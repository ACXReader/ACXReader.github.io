---
title: "Open Thread 301"
subtitle: "..."
date: 2023-11-06
likes: 31
author: Scott Alexander
comments: https://www.astralcodexten.com/api/v1/post/138628601/comments?&all_comments=true
image: https://substack-post-media.s3.amazonaws.com/public/images/b13ae80b-dc68-4288-80fc-951b959a7242_251x255.png
original-url: https://www.astralcodexten.com/p/open-thread-301
---
This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial [subreddit](https://www.reddit.com/r/slatestarcodex/), [Discord](https://discord.gg/RTKtdut), and [bulletin board](https://www.datasecretslox.com/index.php), and [in-person meetups around the world](https://www.lesswrong.com/community?filters%5B0%5D=SSC). 95% of content is free, but for the remaining 5% you can subscribe **[here](https://astralcodexten.substack.com/subscribe?)**. Also:

**1:** MATS (formerly SERI-MATS), a training program for AI alignment research, will be hosting its next cohort from January 17 to March 8 (you would have to be in Berkeley during this period). They “provide talented scholars with talks, workshops, and research mentorship in the field of AI safety”. Application deadline November 10 or 17 depending on exactly what you’re applying for. See [more info here](https://www.matsprogram.org/), [FAQ here](https://www.matsprogram.org/faqs), and [application form here](https://airtable.com/appxum3Sqh7TdDvdg/shrtfHWhRFZdkhaIM).

**2:** I got lots of great responses to the Quests and Requests post. I’ll be contacting some of you individually, and eventually putting the rest into a Highlights From The Comments post so you can see your options and potentially contact each other.
