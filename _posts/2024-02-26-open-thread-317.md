---
title: "Open Thread 317"
subtitle: "..."
date: 2024-02-26
likes: 34
original-url: https://www.astralcodexten.com/p/open-thread-317
---
This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial [subreddit](https://www.reddit.com/r/slatestarcodex/), [Discord](https://discord.gg/RTKtdut), and [bulletin board](https://www.datasecretslox.com/index.php), and [in-person meetups around the world](https://www.lesswrong.com/community?filters%5B0%5D=SSC). 95% of content is free, but for the remaining 5% you can subscribe **[here](https://astralcodexten.substack.com/subscribe?)**. Also:

 **1:** A commenter emailed me to complain that, although I said I unbanned him, the unban never went through. I talked to Substack, who confirmed that _no_ unban has _ever_ gone through, oops, sorry. I’ve found one other case and resolved it, but if I told you I was unbanning you and you aren’t unbanned, email me at scott@slatestarcodex.com and I’ll try to fix it.

 **2:** Related: I have heard your many complaints about page/comment loading speed and passed them on to Substack; my contact there said they’re “pretty sure we've got plans to improve this.”

 **3:** Ben Todd [tried to reproduce my calculations about GPT-6](/p/sam-altman-wants-7-trillion/comment/49693676?r=53mop&utm_campaign=comment-list-share-cta&utm_medium=web&open=false) and found it will only take 0.1% of the world’s computers to train, not 10%. I haven’t double-checked his work or figure out where we disagree, but it sounds like a more reasonable (though still immense) estimate.
