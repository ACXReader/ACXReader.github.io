---
title: "Open Thread 207"
subtitle: "..."
date: 2022-01-17
author: Scott Alexander
comments: https://www.astralcodexten.com/api/v1/post/47229133/comments?&all_comments=true
image: https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/438bb0ae-ff3b-4365-a611-b4a5741b5279_496x341.png
original-url: https://www.astralcodexten.com/p/open-thread-207
---
This is the weekly visible open thread. Odd-numbered open threads will be no-politics, even-numbered threads will be politics-allowed. This one is odd-numbered, so be careful. Otherwise, post about anything else you want. Also:

**1:** The [AI] Alignment Research Center is running the [Eliciting Latent Knowledge contest](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals). They’re awarding between $5,000 and $50,000 (and maybe also job offers) to anyone who can come up with clever ways to get an AI to tell the truth in a contrived hard-to-understand fictional scenario involving a diamond theft. The contest is secretly an attempt to get people in the pipeline of learning about ARC’s ideas and seeing if they’re a good fit for alignment research, and as such, ARC says they’re extremely open to dumb questions, requests for clarification, requests to be walked through certain things, etc. 

Mark Xu of ARC says he would consider someone a “good fit” for alignment research “if they started out with a relatively technical background, e.g. an undergrad degree in math/cs, but not really having engaged with alignment before" and were able to really understand the question in 10-20 hours and have a plausible answer in another 10. 

You can read about the contest [here](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals), and you can read Holden Karnofsky’s pitch for doing it (and attempt to summarize the question) [here](https://forum.effectivealtruism.org/posts/Q2BJnpNh8e6RAWFnm/consider-trying-the-elk-contest-i-am).
