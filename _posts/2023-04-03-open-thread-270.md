---
title: "Open Thread 270"
subtitle: "..."
date: 2023-04-03
author: Scott Alexander
comments: https://www.astralcodexten.com/api/v1/post/112117863/comments?&all_comments=true
image: https://substack-post-media.s3.amazonaws.com/public/images/fcb7bd3a-a8c3-432f-b5d0-67477cf7acb8_255x255.webp
original-url: https://www.astralcodexten.com/p/open-thread-270
---
This is the weekly visible open thread. Post about anything you want, ask random questions, whatever. ACX has an unofficial [subreddit](https://www.reddit.com/r/slatestarcodex/), [Discord](https://discord.gg/RTKtdut), and [bulletin board](https://www.datasecretslox.com/index.php), and [in-person meetups around the world](https://www.lesswrong.com/community?filters%5B0%5D=SSC). 95% of content is free, but for the remaining 5% you can subscribe [here](https://astralcodexten.substack.com/subscribe?). Also:

**1:** Tyler Cowen responded to my post criticizing his AI opinions [here](https://astralcodexten.substack.com/p/mr-tries-the-safe-uncertainty-fallacy/comment/14067796). I responded to his response [here](https://astralcodexten.substack.com/p/mr-tries-the-safe-uncertainty-fallacy/comment/14070813). Since some of the discussion is about whether I misunderstood the original take, you can refresh your memory [here](https://marginalrevolution.com/marginalrevolution/2023/03/existential-risk-and-the-turn-in-human-history.html).

**2:** Reminder: [Book Review Contest](https://astralcodexten.substack.com/p/book-review-contest-rules-2023) entries are due Wednesday, April 5 (you can submit them [here](https://forms.gle/EY5LMzbJQvgYkgxt5)). I won’t be a stickler about time zones, I won’t be a stickler about being a few hours late. No, I won’t tell you how many hours; if I did you’d ask if it was okay to submit it just a few hours later than that. If you try to submit and you find the form is closed, you waited too long.

**3:** Lots of people are looking for trustworthy information about AI safety now. I highly recommend the new blog [Planned Obsolescence](https://www.planned-obsolescence.org/) by Kelsey Piper and Ajeya Cotra, They’re both AI safety veterans, have lots of contacts in industry and research, and are as close to the center of the graph of people thinking about these topics as you’re likely to find. They’re also great writers. Also, the audio version (read by an AI trained to mimic Kelsey’s voice) is very impressive.

**4:** Last month I teamed up with Manifold to run an impact market on forecasting grants. Now Manifold is using their impact market infrastructure, Manifund, to start a market in prizes on [Open Philanthropy’s AI-related essay contest](https://manifund.org/rounds/ai-worldviews?tab=about). The idea is - you write an essay and submit it in hopes of winning (let’s say) the $50,000 first prize. Then you sell the right to the prize on the impact market - for example if you think you’re 10% likely to win (so your essay is worth $5,000) and someone else thinks you’re 20% likely to win (so your essay is worth $10,000), then you could sell the rights to the prize money to them for $7,500 (it’s a bit more complicated than that, but you get the idea). I’m not directly involved in this one, but I trust Manifold a lot and this should help them develop their impact market work further. Yes, you still have to be an accredited investor to buy certificates (though not to sell your essay!). [Go here for more information](https://manifund.org/rounds/ai-worldviews?tab=about). I guess this doubles as an announcement that there’s [an AI-related essay contest](https://manifund.org/rounds/ai-worldviews?tab=about) with a first prize of $50,000. Entries are due May 31 - no, they won’t find it funny if you use GPT.
